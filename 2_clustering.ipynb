{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/user/Projects/thesis/entropic_project/DB_AIRCl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m mapping_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/user/Projects/thesis/entropic_project/DB_AIRCl.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the CSV, skipping the first 2 rows (data starts at row 3).\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Assume:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Column 0: Patient ID, Column 1: Full patient name.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_last_name\u001b[39m(full_name):\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/user/Projects/thesis/entropic_project/DB_AIRCl.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "\n",
    "# --- Load the AIRC database and build the mapping (Cell 1) ---\n",
    "# Define the path to your AIRC CSV.\n",
    "mapping_csv = '/home/user/Projects/thesis/entropic_project/DB_AIRCl.csv'\n",
    "# Load the CSV, skipping the first 2 rows (data starts at row 3).\n",
    "df = pd.read_csv(mapping_csv)\n",
    "\n",
    "\n",
    "# Assume:\n",
    "# Column 0: Patient ID, Column 1: Full patient name.\n",
    "def extract_last_name(full_name):\n",
    "    if ',' in full_name:\n",
    "        return full_name.split(',')[0].strip().lower()\n",
    "    else:\n",
    "        return full_name.split()[0].strip().lower()\n",
    "\n",
    "df['patient_id'] = df.iloc[:, 0].astype(str).str.strip()\n",
    "df['patient_name'] = df.iloc[:, 1].astype(str).str.strip()\n",
    "df['last_name'] = df['patient_name'].apply(extract_last_name)\n",
    "\n",
    "print(\"Patient Mapping DataFrame:\")\n",
    "print(df[['patient_id', 'patient_name', 'last_name']].head())\n",
    "print(df)\n",
    "\n",
    "# Create primary mapping: last_name -> patient_id\n",
    "last_name_to_id = {row['last_name']: row['patient_id'] for _, row in df.iterrows()}\n",
    "\n",
    "# --- Parameters ---\n",
    "kernel_radius_values = [1, 2, 3, 4]\n",
    "base_extraction_dir = '/home/user/Projects/thesis/entropic_project/extraction_results_partial/extraction_results_kernelRadius_'\n",
    "downscale_factor = 4.5  # 4.5 is the limit for my CPU\n",
    "\n",
    "# --- Process files in an extraction folder ---\n",
    "# (Example for one kernelRadius; you can loop over kernel_radius_values as needed.)\n",
    "extraction_folder = f\"{base_extraction_dir}1\"  # For example, kernelRadius=1\n",
    "print(f\"\\n=== Processing extraction folder: {extraction_folder} ===\")\n",
    "\n",
    "tensors = []\n",
    "patient_ids = []\n",
    "\n",
    "for file_name in sorted(os.listdir(extraction_folder)):\n",
    "    if file_name.endswith('.npy'):\n",
    "        file_path = os.path.join(extraction_folder, file_name)\n",
    "        # Use regex to extract a string of letters from the file name.\n",
    "        m = re.search(r'([a-zA-Z]+)', file_name)\n",
    "        if m:\n",
    "            extracted_name = m.group(1).lower()\n",
    "        else:\n",
    "            extracted_name = None\n",
    "\n",
    "        # First, try primary mapping.\n",
    "        if extracted_name and extracted_name in last_name_to_id:\n",
    "            pid = last_name_to_id[extracted_name]\n",
    "        else:\n",
    "            # Fallback: search the full patient_name column for a match.\n",
    "            if extracted_name:\n",
    "                candidates = df[df['patient_name'].str.lower().str.contains(r'\\b' + re.escape(extracted_name))]\n",
    "                if not candidates.empty:\n",
    "                    pid = candidates.iloc[0]['patient_id']\n",
    "                    print(f\"File '{file_name}': extracted '{extracted_name}' matched full name '{candidates.iloc[0]['patient_name']}' with id {pid}\")\n",
    "                else:\n",
    "                    pid = \"Unknown\"\n",
    "                    print(f\"File '{file_name}': extracted '{extracted_name}' not found in full names.\")\n",
    "            else:\n",
    "                pid = \"Unknown\"\n",
    "                print(f\"File '{file_name}': could not extract a name.\")\n",
    "        \n",
    "        patient_ids.append(pid)\n",
    "        \n",
    "        # Load the tensor.\n",
    "        tensor = np.load(file_path)\n",
    "        # Downscale the tensor.\n",
    "        new_shape = tuple(dim // downscale_factor for dim in tensor.shape)\n",
    "        tensor_resized = resize(tensor, new_shape, mode='constant', preserve_range=True, anti_aliasing=True)\n",
    "        tensors.append(torch.tensor(tensor_resized, dtype=torch.float32))\n",
    "\n",
    "print(f\"Loaded {len(tensors)} tensors for {len(patient_ids)} patients.\")\n",
    "print(\"Assigned Patient IDs:\")\n",
    "print(patient_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/user'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m clustering_save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/user/Projects/thesis/entropic_project/clustering_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(clustering_save_dir):\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustering_save_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Helper functions: TensorProduct and IMED (if not already defined)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTensorProduct\u001b[39m(A, B, C):\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "    \u001b[0;31m[... skipping similar frames: makedirs at line 215 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/user'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lifelines.statistics import multivariate_logrank_test\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import os\n",
    "import math\n",
    "from scipy.linalg import eig\n",
    "\n",
    "# --- Clustering Parameters ---\n",
    "sigma_values = [1, 2, 3]\n",
    "num_clusters_list = [2, 3, 4, 5]\n",
    "dbscan_eps = 1\n",
    "dbscan_min_samples = 3\n",
    "\n",
    "clustering_save_dir = \"/home/user/Projects/thesis/entropic_project/clustering_results\"\n",
    "if not os.path.exists(clustering_save_dir):\n",
    "    os.makedirs(clustering_save_dir)\n",
    "\n",
    "# Helper functions: TensorProduct and IMED (if not already defined)\n",
    "def TensorProduct(A, B, C):\n",
    "    return np.kron(np.kron(A, B), C)\n",
    "\n",
    "def IMED(l1, l2, l3, sigma):\n",
    "    square_sigma = sigma ** 2\n",
    "    G1 = np.exp(-np.square(np.arange(l1)[:, None] - np.arange(l1)) / (2 * square_sigma))\n",
    "    G2 = np.exp(-np.square(np.arange(l2)[:, None] - np.arange(l2)) / (2 * square_sigma))\n",
    "    G3 = np.exp(-np.square(np.arange(l3)[:, None] - np.arange(l3)) / (2 * square_sigma))\n",
    "    Gtensor = TensorProduct(G3, G1, G2) * (1 / (2 * math.pi * square_sigma))\n",
    "    vals1, vecs1 = eig(G1)\n",
    "    vals2, vecs2 = eig(G2)\n",
    "    vals3, vecs3 = eig(G3)\n",
    "    Lambda1 = np.diag(vals1.real)\n",
    "    Lambda2 = np.diag(vals2.real)\n",
    "    Lambda3 = np.diag(vals3.real)\n",
    "    Lambdatensor = TensorProduct(Lambda3, Lambda1, Lambda2)\n",
    "    root_Omega = np.sqrt(Lambdatensor)\n",
    "    root_G = math.sqrt(1 / (2 * math.pi * square_sigma)) * (\n",
    "        TensorProduct(vecs3, vecs1, vecs2) @ root_Omega @ TensorProduct(vecs3, vecs1, vecs2).T\n",
    "    )\n",
    "    return Gtensor, root_G\n",
    "\n",
    "# Create a list to store statistical tests results\n",
    "statistical_tests_results = []\n",
    "\n",
    "# --- Main Clustering Loop ---\n",
    "for kr in kernel_radius_values:\n",
    "    # Filter the DataFrame for the current kernel radius.\n",
    "    \n",
    "    print(f\"\\nKernelRadius {kr}: Loaded {len(tensors)} tensors for {len(patient_ids)} patients.\")\n",
    "    \n",
    "    # Pad tensors to a uniform shape.\n",
    "    max_shape = torch.tensor([tensor.shape for tensor in tensors]).max(dim=0).values\n",
    "    padded_tensors = []\n",
    "    for tensor in tensors:\n",
    "        pad_width = [max_shape[i] - tensor.shape[i] for i in range(len(tensor.shape))]\n",
    "        pad_width_reversed = []\n",
    "        for width in pad_width[::-1]:\n",
    "            pad_width_reversed.extend([0, width])\n",
    "        padded_tensor = F.pad(tensor, pad_width_reversed, mode=\"constant\", value=0)\n",
    "        padded_tensors.append(padded_tensor)\n",
    "    print(f\"Padded tensor shape: {padded_tensors[0].shape}\")\n",
    "    \n",
    "    # Flatten padded tensors into a 2D matrix.\n",
    "    n_paz = len(padded_tensors)\n",
    "    vector_length = torch.prod(torch.tensor(padded_tensors[0].shape)).item()\n",
    "    v = torch.zeros((n_paz, vector_length), dtype=torch.float32)\n",
    "    for i, tensor in enumerate(padded_tensors):\n",
    "        v[i] = tensor.flatten()\n",
    "    \n",
    "    # Loop over sigma values.\n",
    "    for sigma in sigma_values:\n",
    "        print(f\"  --- Sigma: {sigma} ---\")\n",
    "        l1, l2, l3 = padded_tensors[0].shape\n",
    "        _, rootG = IMED(l1, l2, l3, sigma)\n",
    "        rootG = torch.tensor(rootG, dtype=torch.float32)\n",
    "        \n",
    "        # Standardize: apply the transformation.\n",
    "        u = torch.zeros_like(v)\n",
    "        for i in range(n_paz):\n",
    "            u[i] = torch.matmul(rootG, v[i])\n",
    "        u_np = u.numpy()\n",
    "        \n",
    "        # Compute Euclidean distance matrix.\n",
    "        distance_matrix = pairwise_distances(u_np, metric=\"euclidean\")\n",
    "        print(f\"Distance matrix shape: {distance_matrix.shape}\")\n",
    "        np.save(f\"/home/user/Projects/thesis/entropic_project/clustering_results/distance_matrix_kRad_{kr}_sigma_{sigma}.npy\", distance_matrix)\n",
    "        print(\"Distance matrix saved.\")\n",
    "        \n",
    "        # Run DBSCAN (once per (kr, sigma)).\n",
    "        dbscan_model = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples, metric='precomputed')\n",
    "        labels_dbscan = dbscan_model.fit_predict(distance_matrix)\n",
    "        \n",
    "        # Loop over target number of clusters.\n",
    "        for n_clusters in num_clusters_list:\n",
    "            print(f\"    >> Clusters: {n_clusters}\")\n",
    "            # Agglomerative Clustering.\n",
    "            agglo = AgglomerativeClustering(n_clusters=n_clusters, metric='precomputed', linkage='average')\n",
    "            labels_agglo = agglo.fit_predict(distance_matrix)\n",
    "            \n",
    "            # KMeans Clustering \n",
    "            kmeans = KMeans(n_clusters=n_clusters, n_init=30, random_state=42)\n",
    "            labels_kmeans = kmeans.fit_predict(distance_matrix)\n",
    "            \n",
    "            # KMedoids Clustering.\n",
    "            kmedoids = KMedoids(n_clusters=n_clusters, metric='precomputed', random_state=42)\n",
    "            labels_kmedoids = kmedoids.fit_predict(distance_matrix)\n",
    "            \n",
    "            # Aggregate clustering results including patient IDs.\n",
    "            clustering_result = {\n",
    "                \"patient_id\": patient_ids,\n",
    "                \"Agglomerative\": labels_agglo,\n",
    "                \"KMeans\": labels_kmeans,\n",
    "                \"KMedoids\": labels_kmedoids,\n",
    "                \"DBSCAN\": labels_dbscan  # DBSCAN labels remain constant for this (kr, sigma)\n",
    "            }\n",
    "            \n",
    "            # Convert to DataFrame.\n",
    "            clustering_df = pd.DataFrame(clustering_result)\n",
    "            print(\"Clustering DataFrame:\")\n",
    "            print(clustering_df)\n",
    "            print(\"CLINICAL DATAFRAME:\")\n",
    "            print(df)\n",
    "            \n",
    "\n",
    "            # Merge with the original DataFrame.\n",
    "            merged_df = df.merge(clustering_df, on=\"patient_id\")\n",
    "            print(\"Merged DataFrame:\")\n",
    "            print(merged_df)\n",
    "\n",
    "\n",
    "            result_filename = f\"clustering_results_kRad_{kr}_sigma_{sigma}_nClusters_{n_clusters}.npy\"\n",
    "            result_filepath = os.path.join(clustering_save_dir, result_filename)\n",
    "            np.save(result_filepath, merged_df)\n",
    "            merged_df.to_csv(f\"/home/user/Projects/thesis/entropic_project/clustering_results/clustering_results_kRad_{kr}_sigma_{sigma}_nClusters_{n_clusters}.csv\", index=False)\n",
    "            print(f\"      Saved clustering result: {result_filepath}\")\n",
    "            print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
